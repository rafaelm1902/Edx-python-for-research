{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.1: Tree-Based Methods for Regression and Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest method makes use of several trees\n",
    "when making its prediction, and since in graph theory, a collection of trees\n",
    "is called a forest, this is where the random forest method gets its name.\n",
    "\n",
    "In other words, to make a prediction, the random forest\n",
    "considers the predictions of several trees.\n",
    "It would not, however, be useful to have many identical trees\n",
    "because all these trees would presumably give you the same prediction.\n",
    "This is why the trees in the forest are randomized\n",
    "\n",
    "These methods involve dividing the predictor space\n",
    "into simpler regions using straight lines.\n",
    "So we first take the entire predictor space and divide it into two regions.\n",
    "We now in turn look at each of these two smaller regions,\n",
    "divide them into yet smaller regions, and so on,\n",
    "continuing until we hit some stopping criteria.\n",
    "\n",
    "In the regression setting, we return the mean\n",
    "of the outcomes of the training observations in that particular region,\n",
    "whereas in a classification setting we return\n",
    "the mode, the most common element of the outcomes of the training\n",
    "observations in that region.\n",
    "\n",
    "In regression, this loss function is usually\n",
    "RSS, the residual sum of squares.\n",
    "In classification, two measures are commonly used,\n",
    "called the Gini index and the cross-entropy.\n",
    "\n",
    "the basic idea is, again, to make cuts using a predictor\n",
    "cut point combination that makes the classes within each region\n",
    "as homogeneous as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of a tree-based method is typically to split up the predictor or feature space such that:\n",
    "Answer: data within each region are as similar as possible.\n",
    "\n",
    "Part 1. For classification, how does a decision tree make a prediction for a new data point?\n",
    "Anwser: It returns the mode of the outcomes of the training data points in the predictor space where the new data point falls.\n",
    "\n",
    "Part 2. For regression, how does a decision tree make a prediction for a new data point?\n",
    "Answer: It returns the mean of the outcomes of the training data points in the predictor space where the new data point falls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3.2: Random Forest Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction of the random forest combines information\n",
    "from the predictions of the individual trees.\n",
    "\n",
    "In the regression setting, the prediction of the random forest\n",
    "is the mean of the predictions of the individual trees.\n",
    "\n",
    "In a classification setting, the prediction of the random forest\n",
    "is the mode of the predictions of the individual trees.\n",
    "\n",
    "Random forests introduce two types of randomness to decision trees.\n",
    "\n",
    "The first type has to do with introducing randomness to the data,\n",
    "so that each tree is fit to a somewhat different dataset.\n",
    "\n",
    "The second type of randomness has to do with which\n",
    "predictors are considered when making a split at any point in a given tree.\n",
    "\n",
    "These two steps have the implication of decorrelating\n",
    "the trees, which ultimately gives us a more reliable method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first type of randomness, randomness in data,\n",
    "is due to bootstrap aggregation, which is often called bagging.\n",
    "Bootstrap is a re-sampling method, which involves repeatedly drawing samples\n",
    "from a training set and refitting a model on each sample.\n",
    "\n",
    "If we have n observations in our training data set,\n",
    "we form a bootstrap dataset by randomly selecting n observations\n",
    "with replacement from the original dataset.\n",
    "Because the sampling is performed with replacement,\n",
    "the same observation can occur multiple times in the bootstrap data.\n",
    "\n",
    "We can perform this process multiple times,\n",
    "and we'll likely get a somewhat different data set every time.\n",
    "So bagging, in the context of decision trees,\n",
    "means that we draw a number of bootstrap datasets and fit each to a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second type of randomness, randomness\n",
    "in how we split the predictor space, happens as follows.\n",
    "\n",
    "Normally with decision trees, we consider each predictor-cut point\n",
    "combination when making a cut into predictor space.\n",
    "\n",
    "In contrast, in random forest, each time we consider a split,\n",
    "we don't look at all predictors, but instead\n",
    "draw a small random sample of the predictors.\n",
    "And now we're only allowed to use these predictors when making a split.\n",
    "\n",
    "Each time we make a split, we take a new sample of predictors.\n",
    "This sounds really strange, but it's actually a very effective trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple example.\n",
    "We start from some dataset having 1,000 observations,\n",
    "and we have 9 predictors from x1 through x9.\n",
    "We want to build, say, 50 trees.\n",
    "So let's randomize the data first.\n",
    "We first draw 50 bootstrap samples from the original data\n",
    "and dedicate a separate tree for each dataset.\n",
    "We then fit the trees one by one.\n",
    "Starting from the first tree and the first cut,\n",
    "we first determine which predictors to use.\n",
    "If we're allowed to use, say, three predictors when making a cut,\n",
    "we might be allowed to use x3, x7, and, say, x8 for the first cut.\n",
    "We make the best cut we can given the data and these three predictors,\n",
    "and we then move the second cut in the first tree.\n",
    "This time we might be allowed to use predictors x1, x5, and x7.\n",
    "And again, we find the best cut.\n",
    "We proceed until we fit the first three, meaning until we fit\n",
    "whatever stopping criterion we have.\n",
    "We then continue the same way until we get all of the trees in the forest.\n",
    "To make a prediction using a random forest,\n",
    "we identify the region in the predicted space\n",
    "separately for each tree, where the test observation happens to fall.\n",
    "Based on this, we next have each tree make a separate prediction.\n",
    "And we then combine the predictions of the individual trees\n",
    "to form the prediction of the forest.\n",
    "So this is how random forests work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the great features of the sklearn library is that there is a consistent framework for the workflow needed to use different statistical models.\n",
    "\n",
    "To do random forest regression, you use the following import:\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "To do random forest classification, you use the following import:\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "After importing the relevant model, everything proceeds in the same way as for linear and logistic regression as shown in the previous videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests get their name by introducing randomness to decision trees in two ways, once at the data level and once at the predictor level.\n",
    "\n",
    "Part 1. How is randomness at the data level introduced?\n",
    "Answer: Each tree gets a bootstrapped random sample of training data.\n",
    "\n",
    "Part 2. How is randomness at the predictor level introduced?\n",
    "answer: Each split only uses a subset of predictors.\n",
    "\n",
    "Part 1. In a classification setting, how does a random forest make predictions?\n",
    "Answer: Each tree makes a prediction and the mode of these predictions is the prediction of the forest.\n",
    "\n",
    "Part 2. In a regression setting, how does a random forest make predictions?\n",
    "Each tree makes a prediction and the mean of these predictions is the prediction of the forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
